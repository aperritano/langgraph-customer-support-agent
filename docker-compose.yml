services:
  # NOTE: Using native Ollama (outside Docker) for Metal GPU acceleration
  # Make sure Ollama is running natively: `ollama serve` or ensure it's running
  # Native Ollama automatically uses Apple Silicon Metal GPU (MPS)

  # Customer support bot
  support-bot:
    build: .
    ports:
      - "8123:8123"
    environment:
      # Connect to native Ollama on host (uses Metal GPU automatically)
      # host.docker.internal works on macOS/Windows Docker Desktop
      - OLLAMA_BASE_URL=http://host.docker.internal:11434
    volumes:
      - ./storage:/app/storage
      - ./src:/app/src
      - ./data:/app/data
    networks:
      - support-network
    restart: unless-stopped

volumes:
  ollama-data:
    driver: local

networks:
  support-network:
    driver: bridge
